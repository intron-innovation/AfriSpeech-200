[experiment]
name = wav2vec2-large-xlsr-53-multi-task_train
repo_root = /data2/lr_scheduler
dir = src/experiments/

[data]
train = data/intron-train-public-58000-clean.csv
val = data/intron-dev-public-3231-clean.csv
domain = all
sampler = LengthGroupedSampler


[models]
model_path = facebook/wav2vec2-large-xlsr-53



[audio]
audio_path = /data3/data/intron/


[checkpoints]
checkpoints_path = True

[hyperparameters]
data_seed=12260
dataset_name=afrispeech 
model_name_or_path=facebook/wav2vec2-large-xlsr-53 
output_dir=./wav2vec2-large-xlsr-53-multi_gpu
overwrite_output_dir=True
per_device_train_batch_size=4 
learning_rate=3e-4 
warmup_steps=400 
evaluation_strategy=steps 
dataloader_num_workers=5
seed=1778
text_column_name=transcript 
length_column_name=duration 
num_epochs=5
save_steps=1000 
eval_steps=1000 
logging_steps=1 
load_best_model_at_end=True
ignore_data_skip=False
ctc_loss_reduction=mean
layerdrop=0.0
save_total_limit=3 
train_batch_size= 16
ddp_find_unused_parameters=False
val_batch_size = 8
gradient_accumulation_steps=2
freeze_feature_encoder=True
ctc_zero_infinity=True
gradient_checkpointing=True
fp16 =True
group_by_length =True
do_train=True
do_eval=True
max_audio_len=260000
max_label_len=260
max_audio_len_secs=17
min_transcript_len=10
attention_dropout=0.1
hidden_dropout=0.1
feat_proj_dropout=0.0
mask_time_prob=0.05
lr_schedule=cyclicLR
max_learning_rate=3e-7


[tasks]
architecture=generative
expand_vocab=True
expand_mode=prepend
accent=False
domain=False
vad=False
loss_reduction=None
alphas=None
num_accents=80
num_domains=3
num_vad=2

[logs]
train_logs = True
figure_path = True
predictions_path = True 
