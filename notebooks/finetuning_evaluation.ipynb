{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7e6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964a1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_metric\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.utils.text_processing import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51efb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998869b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy functions\n",
    "def transform_raw_df(df_raw, mapping_df, split_df, metric, columns=[], fmt=\"intron_whisper\"):\n",
    "    assert fmt != \"\" or fmt is not None\n",
    "\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    print(f\"df shape: {df.shape}\")\n",
    "\n",
    "    df = df[[\"audio_paths\", \"hypothesis\"]]\n",
    "    df = pd.merge(df, mapping_df, on=\"audio_paths\")\n",
    "    df = pd.merge(df, split_df, on=[\"idx\", \"audio_paths\"])\n",
    "    df = df[~df.duplicated(subset=\"idx\")]\n",
    "\n",
    "    df = df[columns+[\"hypothesis\"]]\n",
    "    df.loc[:, \"reference\"] = df.loc[:, \"transcript\"]\n",
    "    df.loc[:, \"prediction\"] = df.loc[:, \"hypothesis\"]\n",
    "    df = df.drop(columns=[\"hypothesis\", \"transcript\"])\n",
    "    \n",
    "    print(f\"df shape (transformed): {df.shape}\")\n",
    "    df[\"wer\"] = df.apply(lambda x: wer_metric.compute(predictions=[x.prediction], references=[x.reference]), axis=1) \n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def normalize_and_compute_wer(df, metric):\n",
    "    normalized_df = df.copy()\n",
    "    print(f\"Total null values: {normalized_df[normalized_df['prediction'].isnull()].shape[0]}\")\n",
    "    normalized_df[\"prediction\"] = normalized_df[\"prediction\"].fillna(\"\")\n",
    "    \n",
    "    normalized_df[\"reference\"] = normalized_df[\"reference\"].apply(lambda x: clean_text(x))\n",
    "    normalized_df[\"prediction\"] = normalized_df[\"prediction\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    normalized_df[\"wer\"] = normalized_df.apply(lambda x: wer_metric.compute(predictions=[x.prediction], \n",
    "                                                                  references=[x.reference]), axis=1)\n",
    "    return normalized_df\n",
    "    \n",
    "    \n",
    "def write_to_folder(model_id_or_path, predictions_df, output_dir=\"../results/\", domain=\"all\", split=\"test\"):\n",
    "    wer = wer_metric.compute(predictions=predictions_df.prediction, references=predictions_df.reference)\n",
    "    print(f\"wer: {wer}\")\n",
    "    output_path = f\"{output_dir}/intron-open-{split}-{model_id_or_path}-wer-{round(wer, 4)}-{len(predictions_df)}.csv\"\n",
    "    print(f\"output path: {output_path}\")\n",
    "    print(f\"Output shape: {predictions_df.shape}\")\n",
    "    predictions_df.to_csv(output_path, index=False)\n",
    "    \n",
    "def consolidate_inference_results(model_csv_path, model_name, metric, domain,\n",
    "                                  ref_csv_path=\"../results/intron-open-test-whisper_medium-wer-0.3322-5474.csv\",\n",
    "                                  dataset_csv_path=\"../data/intron-test-public-6346-clean.csv\",\n",
    "                                  output_dir=\"../results\"):\n",
    "    \n",
    "    print(f\"Input path: {model_csv_path}\")\n",
    "    raw_df = pd.read_csv(model_csv_path)\n",
    "    print(f\"Input shape: {raw_df.shape}\")\n",
    "    \n",
    "    split_df = pd.read_csv(dataset_csv_path)\n",
    "    print(f\"Dataset split shape: {split_df.shape}\")\n",
    "    split_df = split_df[~split_df.duplicated(subset=\"audio_paths\")]\n",
    "    split_df = split_df[~split_df.duplicated(subset=\"idx\")]\n",
    "    print(f\"Dataset split shape (without duplicates): {split_df.shape}\")\n",
    "    split_name = split_df.split.unique()[0]\n",
    "    split_df[\"audio_paths\"] = split_df[\"audio_paths\"].apply(lambda x: x.replace(f\"/AfriSpeech-100/{split_name}/\", \"/data/data/intron/\"))\n",
    "    \n",
    "    \n",
    "    ref_df = pd.read_csv(ref_csv_path)\n",
    "    print(f\"Reference csv shape: {ref_df.shape}\")\n",
    "    ref_df = pd.merge(ref_df, split_df, on=\"audio_paths\")\n",
    "    ref_df[ref_df.duplicated(subset=\"audio_paths\")]\n",
    "    print(f\"Reference csv shape (without duplicates): {ref_df.shape}\")\n",
    "    \n",
    "    mapping_df = ref_df[[\"idx\", \"audio_paths\"]]\n",
    "    columns = [\"idx\", \"domain\", \"gender\", \"duration\", \n",
    "               \"age_group\", \"accent\", \"user_ids\", \n",
    "               \"transcript\", \"audio_paths\", \"origin\",\n",
    "              \"country\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    # create output directory if not exist\n",
    "    os.makedirs(f\"{output_dir}/raw\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/normalized\", exist_ok=True)\n",
    "    \n",
    "    models_name_mapping = {\n",
    "        \"openai_whisper_medium_all\": \"openai/whisper-medium-all\",\n",
    "        \"openai_whisper_medium_general\": \"openai/whisper-medium-general\",\n",
    "        \"openai_whisper_medium_clinical\": \"openai/whisper-medium-clinical\",\n",
    "        \"facebook_wav2vec2_large_xlsr_53_english_all\": \"facebook/wav2vec2-large-xlsr-53-english-all\",\n",
    "        \"facebook_wav2vec2_large_xlsr_53_english_general\": \"facebook/wav2vec2-large-xlsr-53-english-general\",\n",
    "        \"facebook_wav2vec2_large_xlsr_53_english_clinical\": \"facebook/wav2vec2-large-xlsr-53-english-clinical\"\n",
    "    }\n",
    "    \n",
    "   \n",
    "    # transform the raw dataframe\n",
    "    df = transform_raw_df(raw_df, split_df=split_df, mapping_df=mapping_df, columns=columns, metric=metric)\n",
    "    df[\"name\"] = models_name_mapping[model_name]\n",
    "    df[\"split\"] = split_name\n",
    "    df[\"audio_paths\"] = df[\"audio_paths\"].apply(lambda x: x.replace(\"/data/data/intron/\", f\"/AfriSpeech-100/{split_name}/\"))\n",
    "    \n",
    "    print(\"***raw\")\n",
    "    # write the result to folder\n",
    "    write_to_folder(model_id_or_path=model_name, \n",
    "                    predictions_df=df, \n",
    "                    domain=domain,\n",
    "                    output_dir=f\"{output_dir}/raw\",\n",
    "                    split=split_name)\n",
    "    \n",
    "    \n",
    "    # normalize the raw dataframe\n",
    "    normalized_df = normalize_and_compute_wer(df, metric=metric)\n",
    "    normalized_df[\"name\"] = models_name_mapping[model_name]\n",
    "    normalized_df[\"split\"] = split_name\n",
    "    normalized_df[\"audio_paths\"] = normalized_df[\"audio_paths\"].apply(lambda x: x.replace(\"/data/data/intron/\", f\"/AfriSpeech-100/{split_name}/\"))\n",
    "    \n",
    "    # write the result to folder\n",
    "    print(\"***normalized\")\n",
    "    write_to_folder(model_id_or_path=model_name, \n",
    "                    predictions_df=normalized_df,\n",
    "                    domain=domain,\n",
    "                    output_dir=f\"{output_dir}/normalized\",\n",
    "                    split=split_name)\n",
    "    \n",
    "    \n",
    "    return df, normalized_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd74c2",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ebe7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_name_csv_path_dict = {\n",
    "    \"openai_whisper_medium_all\": [\"../results/intron-open-test-whisper_all-wer-0.2161-5474.csv\", \"all\"],\n",
    "    \"openai_whisper_medium_general\": [\"../results/intron-open-test-whisper_general-wer-0.3508-5474.csv\", \"general\"],\n",
    "    \"openai_whisper_medium_clinical\": [\"../results/intron-open-test-whisper_clinical-wer-0.3678-5474.csv\", \"clinical\"],\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english_all\": [\"../results/intron-open-test-wav2vec2-large-xlsr-53-all-wer-0.2931-5474.csv\", \"all\"],\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english_general\": [\"../results/intron-open-test-wav2vec2-large-xlsr-53-general-wer-0.3487-5474.csv\", \"general\"],\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english_clinical\": [\"../results/intron-open-test-wav2vec2-large-xlsr-53-clinical-wer-0.3675-5474.csv\", \"clinical\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ec28cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: openai_whisper_medium_all\n",
      "Input path: ../results/intron-open-test-whisper_all-wer-0.2161-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.3420153211549794\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_medium_all-wer-0.342-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.2161564046827122\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_medium_all-wer-0.2162-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium_general\n",
      "Input path: ../results/intron-open-test-whisper_general-wer-0.3508-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.44872651083611603\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_medium_general-wer-0.4487-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.35087604431291414\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_medium_general-wer-0.3509-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium_clinical\n",
      "Input path: ../results/intron-open-test-whisper_clinical-wer-0.3678-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.4007726052510967\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_medium_clinical-wer-0.4008-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.3677814734305843\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_medium_clinical-wer-0.3678-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english_all\n",
      "Input path: ../results/intron-open-test-wav2vec2-large-xlsr-53-all-wer-0.2931-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.40014404504681467\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_large_xlsr_53_english_all-wer-0.4001-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.29315402142314645\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_large_xlsr_53_english_all-wer-0.2932-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english_general\n",
      "Input path: ../results/intron-open-test-wav2vec2-large-xlsr-53-general-wer-0.3487-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.4402409480783081\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_large_xlsr_53_english_general-wer-0.4402-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.3488332503993924\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_large_xlsr_53_english_general-wer-0.3488-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english_clinical\n",
      "Input path: ../results/intron-open-test-wav2vec2-large-xlsr-53-clinical-wer-0.3675-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.45887513913442024\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_large_xlsr_53_english_clinical-wer-0.4589-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.3675195767750046\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_large_xlsr_53_english_clinical-wer-0.3675-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Combined df shape: (32826, 15)\n",
      "Combined df (normalized) shape: (32826, 15)\n"
     ]
    }
   ],
   "source": [
    "test_combined_df = pd.DataFrame()\n",
    "test_combined_normalized_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model_csv_path_and_domain in test_model_name_csv_path_dict.items():\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    df, normalized_df = consolidate_inference_results(\n",
    "        model_csv_path=model_csv_path_and_domain[0], \n",
    "        model_name=model_name,\n",
    "        metric=wer_metric,\n",
    "        domain=model_csv_path_and_domain[1],\n",
    "        ref_csv_path=\"../results/intron-open-test-whisper_medium-wer-0.3322-5474.csv\",\n",
    "        dataset_csv_path=\"../data/intron-test-public-6346-clean.csv\",\n",
    "        output_dir=\"../results\", \n",
    "    )\n",
    "\n",
    "\n",
    "    test_combined_df = pd.concat([test_combined_df, df])\n",
    "    test_combined_normalized_df = pd.concat([test_combined_normalized_df, normalized_df])\n",
    "    print(\"=\"*10)\n",
    "\n",
    "print(f\"Combined df shape: {test_combined_df.shape}\")\n",
    "print(f\"Combined df (normalized) shape: {test_combined_normalized_df.shape}\")\n",
    "\n",
    "test_combined_df.to_csv(\"../results/raw/intron-open-test-all_models_finetuned.csv\", index=False)\n",
    "test_combined_normalized_df.to_csv(\"../results/normalized/intron-open-test-all_models_finetuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1df7c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>domain</th>\n",
       "      <th>gender</th>\n",
       "      <th>duration</th>\n",
       "      <th>age_group</th>\n",
       "      <th>accent</th>\n",
       "      <th>user_ids</th>\n",
       "      <th>audio_paths</th>\n",
       "      <th>origin</th>\n",
       "      <th>country</th>\n",
       "      <th>reference</th>\n",
       "      <th>prediction</th>\n",
       "      <th>wer</th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149716</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>10.776984</td>\n",
       "      <td>26-40</td>\n",
       "      <td>twi</td>\n",
       "      <td>5ab7b49ad1cab6392a764a69578dc822</td>\n",
       "      <td>/AfriSpeech-100/test/e696aff8-ce05-4c8e-a3b3-5...</td>\n",
       "      <td>african</td>\n",
       "      <td>GH</td>\n",
       "      <td>Proteins break down to release amino acids whi...</td>\n",
       "      <td>proteins breakdown to release amino acids whic...</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>openai/whisper-medium-all</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360395</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>12.390000</td>\n",
       "      <td>19-25</td>\n",
       "      <td>igbo</td>\n",
       "      <td>543c037ff44816e8b5ccd0d6cc92fe13</td>\n",
       "      <td>/AfriSpeech-100/test/f28baac3-cdcd-45a2-888c-a...</td>\n",
       "      <td>nigerian</td>\n",
       "      <td>NG</td>\n",
       "      <td>To grant such a patent license to a party mean...</td>\n",
       "      <td>togrant such a patent license to a party means...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>openai/whisper-medium-all</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153514</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>5.826984</td>\n",
       "      <td>26-40</td>\n",
       "      <td>isizulu</td>\n",
       "      <td>e87486db0c365bded42184d56b46a2a7</td>\n",
       "      <td>/AfriSpeech-100/test/13062a1b-662b-4afa-85b4-9...</td>\n",
       "      <td>african</td>\n",
       "      <td>ZA</td>\n",
       "      <td>Flatus indicates return of peristalsis.</td>\n",
       "      <td>flatus indicates return ofperistalsis.</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>openai/whisper-medium-all</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx    domain  gender   duration age_group   accent  \\\n",
       "0  149716  clinical  Female  10.776984     26-40      twi   \n",
       "1  360395  clinical  Female  12.390000     19-25     igbo   \n",
       "2  153514  clinical  Female   5.826984     26-40  isizulu   \n",
       "\n",
       "                           user_ids  \\\n",
       "0  5ab7b49ad1cab6392a764a69578dc822   \n",
       "1  543c037ff44816e8b5ccd0d6cc92fe13   \n",
       "2  e87486db0c365bded42184d56b46a2a7   \n",
       "\n",
       "                                         audio_paths    origin country  \\\n",
       "0  /AfriSpeech-100/test/e696aff8-ce05-4c8e-a3b3-5...   african      GH   \n",
       "1  /AfriSpeech-100/test/f28baac3-cdcd-45a2-888c-a...  nigerian      NG   \n",
       "2  /AfriSpeech-100/test/13062a1b-662b-4afa-85b4-9...   african      ZA   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Proteins break down to release amino acids whi...   \n",
       "1  To grant such a patent license to a party mean...   \n",
       "2            Flatus indicates return of peristalsis.   \n",
       "\n",
       "                                          prediction       wer  \\\n",
       "0  proteins breakdown to release amino acids whic...  0.391304   \n",
       "1  togrant such a patent license to a party means...  0.160000   \n",
       "2             flatus indicates return ofperistalsis.  0.600000   \n",
       "\n",
       "                        name split  \n",
       "0  openai/whisper-medium-all  test  \n",
       "1  openai/whisper-medium-all  test  \n",
       "2  openai/whisper-medium-all  test  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a24496",
   "metadata": {},
   "source": [
    "# Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cce980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_model_name_csv_path_dict = {\n",
    "    \"openai_whisper_medium_all\": [\"../results/intron-open-dev-whisper_all-wer-0.2272-2883.csv\", \"all\"],\n",
    "    \"openai_whisper_medium_general\": [\"../results/intron-open-dev-whisper_general-wer-0.3469-2883.csv\", \"general\"],\n",
    "    \"openai_whisper_medium_clinical\": [\"../results/intron-open-dev-whisper_clinical-wer-0.3763-2883.csv\", \"clinical\"],\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english_all\": [\"../results/intron-open-dev-wav2vec2-large-xlsr-53-all-wer-0.3017-2883.csv\", \"all\"],\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english_general\": [\"../results/intron-open-dev-wav2vec2-large-xlsr-53-general-wer-0.3468-2883.csv\", \"general\"],\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english_clinical\": [\"../results/intron-open-dev-wav2vec2-large-xlsr-53-clinical-wer-0.3739-2883.csv\", \"clinical\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b6edd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: openai_whisper_medium_all\n",
      "Input path: ../results/intron-open-dev-whisper_all-wer-0.2272-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.3351252376816357\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_medium_all-wer-0.3351-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.226967505634861\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_medium_all-wer-0.227-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium_general\n",
      "Input path: ../results/intron-open-dev-whisper_general-wer-0.3469-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.43684124040470435\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_medium_general-wer-0.4368-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.3467317806160781\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_medium_general-wer-0.3467-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium_clinical\n",
      "Input path: ../results/intron-open-dev-whisper_clinical-wer-0.3763-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.39935209746707667\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_medium_clinical-wer-0.3994-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.376080015026296\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_medium_clinical-wer-0.3761-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english_all\n",
      "Input path: ../results/intron-open-dev-wav2vec2-large-xlsr-53-all-wer-0.3017-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.39388248550435456\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english_all-wer-0.3939-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.3015589782118708\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english_all-wer-0.3016-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english_general\n",
      "Input path: ../results/intron-open-dev-wav2vec2-large-xlsr-53-general-wer-0.3468-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.43059696237000866\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english_general-wer-0.4306-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.3465909090909091\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english_general-wer-0.3466-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english_clinical\n",
      "Input path: ../results/intron-open-dev-wav2vec2-large-xlsr-53-clinical-wer-0.3739-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.4510434517242189\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english_clinical-wer-0.451-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.3736147633358377\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english_clinical-wer-0.3736-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Combined df shape: (17232, 15)\n",
      "Combined df (normalized) shape: (17232, 15)\n"
     ]
    }
   ],
   "source": [
    "dev_combined_df = pd.DataFrame()\n",
    "dev_combined_normalized_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model_csv_path_and_domain in dev_model_name_csv_path_dict.items():\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    df, normalized_df = consolidate_inference_results(\n",
    "        model_csv_path=model_csv_path_and_domain[0], \n",
    "        model_name=model_name,\n",
    "        metric=wer_metric,\n",
    "        domain=model_csv_path_and_domain[1],\n",
    "        ref_csv_path=\"../results/intron-open-dev-whisper_medium-wer-0.3049-2872.csv\",\n",
    "        dataset_csv_path=\"../data/intron-dev-public-3231-clean.csv\",\n",
    "        output_dir=\"../results\", \n",
    "    )\n",
    "\n",
    "\n",
    "    dev_combined_df = pd.concat([dev_combined_df, df])\n",
    "    dev_combined_normalized_df = pd.concat([dev_combined_normalized_df, normalized_df])\n",
    "    print(\"=\"*10)\n",
    "\n",
    "print(f\"Combined df shape: {dev_combined_df.shape}\")\n",
    "print(f\"Combined df (normalized) shape: {dev_combined_normalized_df.shape}\")\n",
    "\n",
    "dev_combined_df.to_csv(\"../results/raw/intron-open-dev-all_models_finetuned.csv\", index=False)\n",
    "dev_combined_normalized_df.to_csv(\"../results/normalized/intron-open-dev-all_models_finetuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac27a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
