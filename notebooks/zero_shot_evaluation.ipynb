{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7e6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964a1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_metric\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.utils.text_processing import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51efb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86d2ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>user_ids</th>\n",
       "      <th>accent</th>\n",
       "      <th>age_group</th>\n",
       "      <th>country</th>\n",
       "      <th>transcript</th>\n",
       "      <th>nchars</th>\n",
       "      <th>audio_ids</th>\n",
       "      <th>audio_paths</th>\n",
       "      <th>duration</th>\n",
       "      <th>origin</th>\n",
       "      <th>domain</th>\n",
       "      <th>split</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149716</td>\n",
       "      <td>5ab7b49ad1cab6392a764a69578dc822</td>\n",
       "      <td>twi</td>\n",
       "      <td>26-40</td>\n",
       "      <td>GH</td>\n",
       "      <td>Proteins break down to release amino acids whi...</td>\n",
       "      <td>136</td>\n",
       "      <td>3a21d00eec39a31f089b9b4f0f8afa21</td>\n",
       "      <td>/AfriSpeech-100/test/e696aff8-ce05-4c8e-a3b3-5...</td>\n",
       "      <td>10.776984</td>\n",
       "      <td>african</td>\n",
       "      <td>clinical</td>\n",
       "      <td>test</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360395</td>\n",
       "      <td>543c037ff44816e8b5ccd0d6cc92fe13</td>\n",
       "      <td>igbo</td>\n",
       "      <td>19-25</td>\n",
       "      <td>NG</td>\n",
       "      <td>To grant such a patent license to a party mean...</td>\n",
       "      <td>128</td>\n",
       "      <td>544fbff921d13b224adbbb0f637196ed</td>\n",
       "      <td>/AfriSpeech-100/test/f28baac3-cdcd-45a2-888c-a...</td>\n",
       "      <td>12.390000</td>\n",
       "      <td>nigerian</td>\n",
       "      <td>clinical</td>\n",
       "      <td>test</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx                          user_ids accent age_group country  \\\n",
       "0  149716  5ab7b49ad1cab6392a764a69578dc822    twi     26-40      GH   \n",
       "1  360395  543c037ff44816e8b5ccd0d6cc92fe13   igbo     19-25      NG   \n",
       "\n",
       "                                          transcript  nchars  \\\n",
       "0  Proteins break down to release amino acids whi...     136   \n",
       "1  To grant such a patent license to a party mean...     128   \n",
       "\n",
       "                          audio_ids  \\\n",
       "0  3a21d00eec39a31f089b9b4f0f8afa21   \n",
       "1  544fbff921d13b224adbbb0f637196ed   \n",
       "\n",
       "                                         audio_paths   duration    origin  \\\n",
       "0  /AfriSpeech-100/test/e696aff8-ce05-4c8e-a3b3-5...  10.776984   african   \n",
       "1  /AfriSpeech-100/test/f28baac3-cdcd-45a2-888c-a...  12.390000  nigerian   \n",
       "\n",
       "     domain split  gender  \n",
       "0  clinical  test  Female  \n",
       "1  clinical  test  Female  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../data/intron-test-public-6346-clean.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998869b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy functions\n",
    "def transform_raw_df(df_raw, mapping_df, split_df, metric, columns=[], fmt=\"intron_whisper\"):\n",
    "    assert fmt != \"\" or fmt is not None\n",
    "\n",
    "    \n",
    "    df = df_raw.copy()\n",
    "    print(f\"df shape: {df.shape}\")\n",
    "\n",
    "    if fmt == \"intron_whisper\":\n",
    "        df = df[[\"audio_paths\", \"hypothesis\"]]\n",
    "        df = pd.merge(df, mapping_df, on=\"audio_paths\")\n",
    "        df = pd.merge(df, split_df, on=[\"idx\", \"audio_paths\"])\n",
    "        df = df[~df.duplicated(subset=\"idx\")]\n",
    "\n",
    "        df = df[columns+[\"hypothesis\"]]\n",
    "        df.loc[:, \"reference\"] = df.loc[:, \"transcript\"]\n",
    "        df.loc[:, \"prediction\"] = df.loc[:, \"hypothesis\"]\n",
    "        df = df.drop(columns=[\"hypothesis\", \"transcript\"])\n",
    "    \n",
    "    elif fmt == \"aws\":\n",
    "        df = df[[\"idx\", \"predictions\"]]\n",
    "        df = pd.merge(df, mapping_df, on=\"idx\")\n",
    "        df = pd.merge(df, split_df, on=[\"idx\", \"audio_paths\"])\n",
    "        df = df[~df.duplicated(subset=\"idx\")]\n",
    "\n",
    "        df = df[columns+[\"predictions\"]]\n",
    "        df.loc[:, \"reference\"] = df.loc[:, \"transcript\"]\n",
    "        df.loc[:, \"prediction\"] = df.loc[:, \"predictions\"]\n",
    "        df = df.drop(columns=[\"predictions\",  \"transcript\"])\n",
    "\n",
    "    elif fmt == \"azure\":\n",
    "        df = df[[\"idx\", \"predictions_raw\"]]\n",
    "        df = pd.merge(df, mapping_df, on=\"idx\")\n",
    "        df = pd.merge(df, split_df, on=[\"idx\", \"audio_paths\"])\n",
    "        df = df[~df.duplicated(subset=\"idx\")]\n",
    "\n",
    "        df = df[columns+[\"predictions_raw\"]]\n",
    "        df.loc[:, \"reference\"] = df.loc[:, \"transcript\"]\n",
    "        df.loc[:, \"prediction\"] = df.loc[:, \"predictions_raw\"]\n",
    "        df = df.drop(columns=[\"predictions_raw\",  \"transcript\"])\n",
    "    \n",
    "    elif fmt == \"african_nlp\":\n",
    "        df[\"audio_paths\"] = df[0].apply(lambda x: x.replace(\"/scratch/pbsjobs/axy327/dev/\", \"/data/data/intron/\"))\n",
    "        df = df[[\"audio_paths\", 1]]\n",
    "        df = pd.merge(df, mapping_df, on=\"audio_paths\")\n",
    "        df = pd.merge(df, split_df, on=[\"idx\", \"audio_paths\"])\n",
    "\n",
    "        df = df[columns+[1]]\n",
    "        df.loc[:, \"reference\"] = df.loc[:, \"transcript\"]\n",
    "        df.loc[:, \"prediction\"] = df.loc[:, 1]\n",
    "        df = df.drop(columns=[1,  \"transcript\"])\n",
    "    \n",
    "    elif fmt in df.columns:\n",
    "        df = df[[\"idx\", fmt]]\n",
    "        df = pd.merge(df, mapping_df, on=\"idx\")\n",
    "        df = pd.merge(df, split_df, on=[\"idx\", \"audio_paths\"])\n",
    "        df = df[~df.duplicated(subset=\"idx\")]\n",
    "        \n",
    "        df = df[columns+[fmt]]\n",
    "        df.loc[:, \"reference\"] = df.loc[:, \"transcript\"]\n",
    "        df.loc[:, \"prediction\"] = df.loc[:, fmt]\n",
    "        df = df.drop(columns=[fmt,  \"transcript\"])\n",
    "  \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    print(f\"df shape (transformed): {df.shape}\")\n",
    "    df[\"wer\"] = df.apply(lambda x: wer_metric.compute(predictions=[x.prediction], references=[x.reference]), axis=1) \n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def normalize_and_compute_wer(df, metric):\n",
    "    normalized_df = df.copy()\n",
    "    print(f\"Total null values: {normalized_df[normalized_df['prediction'].isnull()].shape[0]}\")\n",
    "    normalized_df[\"prediction\"] = normalized_df[\"prediction\"].fillna(\"\")\n",
    "    \n",
    "    normalized_df[\"reference\"] = normalized_df[\"reference\"].apply(lambda x: clean_text(x))\n",
    "    normalized_df[\"prediction\"] = normalized_df[\"prediction\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    normalized_df[\"wer\"] = normalized_df.apply(lambda x: wer_metric.compute(predictions=[x.prediction], \n",
    "                                                                  references=[x.reference]), axis=1)\n",
    "    return normalized_df\n",
    "    \n",
    "    \n",
    "def write_to_folder(model_id_or_path, predictions_df, output_dir=\"../results/\", split=\"test\"):\n",
    "    wer = wer_metric.compute(predictions=predictions_df.prediction, references=predictions_df.reference)\n",
    "    print(f\"wer: {wer}\")\n",
    "    output_path = f\"{output_dir}/intron-open-{split}-{model_id_or_path}-wer-{round(wer, 4)}-{len(predictions_df)}.csv\"\n",
    "    print(f\"output path: {output_path}\")\n",
    "    print(f\"Output shape: {predictions_df.shape}\")\n",
    "    predictions_df.to_csv(output_path, index=False)\n",
    "    \n",
    "def consolidate_zero_shot_results(model_csv_path, model_name, metric,\n",
    "                                  ref_csv_path=\"../results/intron-open-test-whisper_medium-wer-0.3322-5474.csv\",\n",
    "                                  dataset_csv_path=\"../data/intron-test-public-6346-clean.csv\",\n",
    "                                  output_dir=\"../results\"):\n",
    "    \n",
    "    split_df = pd.read_csv(dataset_csv_path)\n",
    "    print(f\"Dataset split shape: {split_df.shape}\")\n",
    "    split_df = split_df[~split_df.duplicated(subset=\"audio_paths\")]\n",
    "    split_df = split_df[~split_df.duplicated(subset=\"idx\")]\n",
    "    print(f\"Dataset split shape (without duplicates): {split_df.shape}\")\n",
    "    split_name = split_df.split.unique()[0]\n",
    "    split_df[\"audio_paths\"] = split_df[\"audio_paths\"].apply(lambda x: x.replace(f\"/AfriSpeech-100/{split_name}/\", \"/data/data/intron/\"))\n",
    "    \n",
    "    \n",
    "    print(f\"Input path: {model_csv_path}\")\n",
    "    if model_name in [\"nvidia_nemo_conformer_ctc_large\", \n",
    "                      \"nvidia_nemo_conformer_transducer_large\", \n",
    "                      \"speechbrain_crdnn_rnnlm_librispeech\"] and split_name == \"dev\":\n",
    "        raw_df = pd.read_csv(model_csv_path, header=None, delimiter=\"\t\")\n",
    "        print(f\"Input shape: {raw_df.shape}\")\n",
    "        \n",
    "    else:\n",
    "        raw_df = pd.read_csv(model_csv_path)\n",
    "        print(f\"Input shape: {raw_df.shape}\")\n",
    "    \n",
    "    \n",
    "    ref_df = pd.read_csv(ref_csv_path)\n",
    "    print(f\"Reference csv shape: {ref_df.shape}\")\n",
    "    ref_df = pd.merge(ref_df, split_df, on=\"audio_paths\")\n",
    "    ref_df[ref_df.duplicated(subset=\"audio_paths\")]\n",
    "    print(f\"Reference csv shape (without duplicates): {ref_df.shape}\")\n",
    "    \n",
    "    mapping_df = ref_df[[\"idx\", \"audio_paths\"]]\n",
    "    columns = [\"idx\", \"domain\", \"gender\", \"duration\", \n",
    "               \"age_group\", \"accent\", \"user_ids\", \n",
    "               \"transcript\", \"audio_paths\", \"origin\", \n",
    "               \"country\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    # create output directory if not exist\n",
    "    os.makedirs(f\"{output_dir}/raw\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/normalized\", exist_ok=True)\n",
    "    \n",
    "    models_name_mapping = {\n",
    "        \"openai_whisper_small\": \"openai/whisper-small\",\n",
    "        \"openai_whisper_small_en\": \"openai/whisper-small-en\",\n",
    "        \"openai_whisper_medium\": \"openai/whisper-medium\",\n",
    "        \"openai_whisper_medium_en\": \"openai/whisper-medium-en\",\n",
    "        \"openai_whisper_large\": \"openai/whisper-large\",\n",
    "        \"facebook_hubert_large_ls960_ft\": \"facebook/hubert-large-ls960-ft\",\n",
    "        \"facebook_hubert_xlarge_ls960_ft\": \"facebook/hubert-xlarge-ls960-ft\",\n",
    "        \"facebook_wav2vec2_large_robust_ft_swbd_300h\": \"facebook/wav2vec2-large-robust-ft-swbd-300h\",\n",
    "        \"facebook_wav2vec2_large_xlsr_53_english\": \"facebook/wav2vec2-large-xlsr-53-english\",\n",
    "        \"facebook_wav2vec2_large_960h_lv60_self\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
    "        \"facebook_wav2vec2_large_960h\": \"facebook/wav2vec2-large-960h\",\n",
    "        \"facebook_wav2vec2_xls_r_1b_english\": \"facebook/wav2vec2-xls-r-1b-english\",\n",
    "        \"nvidia_nemo_conformer_ctc_large\": \"nvidia/nemo-conformer-ctc-large\",\n",
    "        \"nvidia_nemo_conformer_transducer_large\": \"nvidia/nemo-conformer-transducer-large\",\n",
    "        \"microsoft_wavlm_libri_clean_100h_base\": \"microsoft/wavlm-libri-clean-100h-base\",\n",
    "        \"microsoft_wavlm_libri_clean_100h_large\": \"microsoft/wavlm-libri-clean-100h-large\",\n",
    "        \"microsoft_azure_speech_api\": \"Azure\",\n",
    "        \"google_gcp_speech_api\": \"GCP\",\n",
    "        \"google_gcp_medical_speech_api\": \"GCP [Medical]\",\n",
    "        \"amazon_aws_transcribe_api\": \"AWS\",\n",
    "        \"amazon_aws_transcribe_medical_api_primary_care\": \"AWS [Medical] (Primary Care)\",\n",
    "        \"speechbrain_crdnn_rnnlm_librispeech\": \"speechbrain/crdnn-rnnlm-librispeech\",  \n",
    "    }\n",
    "    \n",
    "    \n",
    "#     if  model_name == \"openai_whisper_large\" and split_name == \"dev\":\n",
    "#         fmt = \"whisper_large\"\n",
    "        \n",
    "#     elif  model_name == \"google_gcp_speech_api\" and split_name == \"dev\":\n",
    "#         fmt = \"gcp\"\n",
    "    \n",
    "    if  model_name in [\"nvidia_nemo_conformer_ctc_large\", \n",
    "                      \"nvidia_nemo_conformer_transducer_large\", \n",
    "                      \"speechbrain_crdnn_rnnlm_librispeech\"] and split_name == \"dev\":\n",
    "            fmt = \"african_nlp\"\n",
    "    elif model_name in [\"openai_whisper_small\", \n",
    "                      \"openai_whisper_small_en\", \n",
    "                      \"openai_whisper_medium\", \n",
    "                      \"openai_whisper_medium_en\", \n",
    "                      \"openai_whisper_large\", \n",
    "                      \"facebook_hubert_large_ls960_ft\", \n",
    "                      \"facebook_hubert_xlarge_ls960_ft\", \n",
    "                      \"facebook_wav2vec2_large_robust_ft_swbd_300h\", \n",
    "                      \"facebook_wav2vec2_large_xlsr_53_english\", \n",
    "                      \"facebook_wav2vec2_large_960h_lv60_self\", \n",
    "                      \"facebook_wav2vec2_large_960h\", \n",
    "                      \"facebook_wav2vec2_xls_r_1b_english\", \n",
    "                      \"nvidia_nemo_conformer_ctc_large\", \n",
    "                      \"nvidia_nemo_conformer_transducer_large\", \n",
    "                      \"microsoft_wavlm_libri_clean_100h_base\", \n",
    "                      \"microsoft_wavlm_libri_clean_100h_large\",\n",
    "                      \"speechbrain_crdnn_rnnlm_librispeech\"]:\n",
    "        fmt = \"intron_whisper\"\n",
    "    elif model_name in [\"amazon_aws_transcribe_api\", \n",
    "                        \"amazon_aws_transcribe_medical_api_primary_care\",\n",
    "                        \"google_gcp_speech_api\",\n",
    "                        \"google_gcp_medical_speech_api\"]:\n",
    "        fmt = \"aws\"\n",
    "        \n",
    "    elif model_name == \"microsoft_azure_speech_api\":\n",
    "        fmt = \"azure\"\n",
    "    else:\n",
    "        fmt = model_name\n",
    "   \n",
    "    # transform the raw dataframe\n",
    "    df = transform_raw_df(raw_df, split_df=split_df, mapping_df=mapping_df, columns=columns, fmt=fmt, metric=metric)\n",
    "    df[\"name\"] = models_name_mapping[model_name]\n",
    "    df[\"split\"] = split_name\n",
    "    df[\"audio_paths\"] = df[\"audio_paths\"].apply(lambda x: x.replace(\"/data/data/intron/\", f\"/AfriSpeech-100/{split_name}/\"))\n",
    "    \n",
    "    print(\"***raw\")\n",
    "    # write the result to folder\n",
    "    write_to_folder(model_id_or_path=model_name, \n",
    "                    predictions_df=df, \n",
    "                    output_dir=f\"{output_dir}/raw\",\n",
    "                    split=split_name)\n",
    "    \n",
    "    \n",
    "    # normalize the raw dataframe\n",
    "    normalized_df = normalize_and_compute_wer(df, metric=metric)\n",
    "    normalized_df[\"name\"] = models_name_mapping[model_name]\n",
    "    normalized_df[\"split\"] = split_name\n",
    "    normalized_df[\"audio_paths\"] = normalized_df[\"audio_paths\"].apply(lambda x: x.replace(\"/data/data/intron/\", f\"/AfriSpeech-100/{split_name}/\"))\n",
    "    \n",
    "    # write the result to folder\n",
    "    print(\"***normalized\")\n",
    "    write_to_folder(model_id_or_path=model_name, \n",
    "                    predictions_df=normalized_df,\n",
    "                    output_dir=f\"{output_dir}/normalized\",\n",
    "                    split=split_name)\n",
    "    \n",
    "    \n",
    "    return df, normalized_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd74c2",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44ffe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_name_csv_path_dict = {\n",
    "    \"openai_whisper_small\": \"../results/intron-open-test-whisper_small-wer-0.3907-5474.csv\",\n",
    "    \"openai_whisper_small_en\": \"../results/intron-open-test-whisper_small.en-wer-0.4142-5474.csv\",\n",
    "    \"openai_whisper_medium\": \"../results/intron-open-test-whisper_medium-wer-0.3322-5474.csv\",\n",
    "    \"openai_whisper_medium_en\": \"../results/intron-open-test-whisper_medium.en-wer-0.3577-5474.csv\",\n",
    "    \"openai_whisper_large\": \"../results/intron-open-test-whisper_large-wer-0.3057-5474.csv\",\n",
    "    \"facebook_hubert_large_ls960_ft\": \"../results/intron-open-test-facebook-hubert-large-ls960-ft-wer-0.633-5474.csv\",\n",
    "    \"facebook_hubert_xlarge_ls960_ft\": \"../results/intron-open-test-facebook-hubert-xlarge-ls960-ft-wer-0.6409-5474.csv\",\n",
    "    \"facebook_wav2vec2_large_robust_ft_swbd_300h\": \"../results/intron-open-test-facebook-wav2vec2-large-robust-ft-swbd-300h-wer-0.8169-5474.csv\",\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english\": \"../results/intron-open-test-jonatasgrosman-wav2vec2-large-xlsr-53-english-wer-0.576-5474.csv\",\n",
    "    \"facebook_wav2vec2_large_960h_lv60_self\": \"../results/intron-open-test-facebook-wav2vec2-large-960h-lv60-self-wer-0.6111-5474.csv\",\n",
    "    \"facebook_wav2vec2_large_960h\": \"../results/intron-open-test-facebook-wav2vec2-large-960h-wer-0.7169-5474.csv\",\n",
    "    \"facebook_wav2vec2_xls_r_1b_english\": \"../results/intron-open-test-jonatasgrosman-wav2vec2-xls-r-1b-english-wer-0.5935-5474.csv\",\n",
    "    \"microsoft_wavlm_libri_clean_100h_base\": \"../results/intron-open-test-patrickvonplaten-wavlm-libri-clean-100h-base-plus-wer-0.8593-5474.csv\",\n",
    "    \"microsoft_wavlm_libri_clean_100h_large\": \"../results/intron-open-test-patrickvonplaten-wavlm-libri-clean-100h-large-wer-0.7051-5474.csv\",\n",
    "    \"microsoft_azure_speech_api\": \"../results/intron-open-test-azure-transcribe-wer-0.4437-5498.csv\",\n",
    "    \"google_gcp_speech_api\": \"../results/intron-open-test-gcp-transcribe-wer-0.6357-5498.csv\",\n",
    "    \"google_gcp_medical_speech_api\": \"../results/intron-open-test-gcp-transcribe-medical-wer-0.625-5498.csv\",\n",
    "    \"amazon_aws_transcribe_api\": \"../results/intron-open-test-aws-transcribe-wer-0.5417-5498.csv\",\n",
    "    \"amazon_aws_transcribe_medical_api_primary_care\": \"../results/intron-open-test-aws-transcribe-medical-wer-0.5682-5498.csv\",\n",
    "\n",
    "#     \"speechbrain_crdnn_rnnlm_librispeech\": \"\",  \n",
    "#     \"nvidia_nemo_conformer_ctc_large\": \"\",\n",
    "#     \"nvidia_nemo_conformer_transducer_large\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87127e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: openai_whisper_small\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-whisper_small-wer-0.3907-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.4812937864204806\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_small-wer-0.4813-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.39069743079380875\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_small-wer-0.3907-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: openai_whisper_small_en\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-whisper_small.en-wer-0.4142-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.48898055391868\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_small_en-wer-0.489-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.4142812246287615\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_small_en-wer-0.4143-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-whisper_medium-wer-0.3322-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.4334315458652524\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_medium-wer-0.4334-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.33218971793730195\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_medium-wer-0.3322-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium_en\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-whisper_medium.en-wer-0.3577-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.43918025273358213\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_medium_en-wer-0.4392-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.3576067883613126\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_medium_en-wer-0.3576-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: openai_whisper_large\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-whisper_large-wer-0.3057-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.3900216067570222\n",
      "output path: ../results/raw/intron-open-test-openai_whisper_large-wer-0.39-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.30579053505486736\n",
      "output path: ../results/normalized/intron-open-test-openai_whisper_large-wer-0.3058-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_hubert_large_ls960_ft\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-facebook-hubert-large-ls960-ft-wer-0.633-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 1.160701892228115\n",
      "output path: ../results/raw/intron-open-test-facebook_hubert_large_ls960_ft-wer-1.1607-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.6330958803656077\n",
      "output path: ../results/normalized/intron-open-test-facebook_hubert_large_ls960_ft-wer-0.6331-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_hubert_xlarge_ls960_ft\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-facebook-hubert-xlarge-ls960-ft-wer-0.6409-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 1.1814050939566556\n",
      "output path: ../results/raw/intron-open-test-facebook_hubert_xlarge_ls960_ft-wer-1.1814-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.6410444438624519\n",
      "output path: ../results/normalized/intron-open-test-facebook_hubert_xlarge_ls960_ft-wer-0.641-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_robust_ft_swbd_300h\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-facebook-wav2vec2-large-robust-ft-swbd-300h-wer-0.8169-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 1.2319387153800825\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_large_robust_ft_swbd_300h-wer-1.2319-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 4\n",
      "***normalized\n",
      "wer: 0.8169997119136788\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_large_robust_ft_swbd_300h-wer-0.817-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-jonatasgrosman-wav2vec2-large-xlsr-53-english-wer-0.576-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.6464217900870818\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_large_xlsr_53_english-wer-0.6464-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.5760547887803473\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_large_xlsr_53_english-wer-0.5761-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_960h_lv60_self\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-facebook-wav2vec2-large-960h-lv60-self-wer-0.6111-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 1.1655470438027893\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_large_960h_lv60_self-wer-1.1655-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.6111620354608072\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_large_960h_lv60_self-wer-0.6112-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_960h\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-facebook-wav2vec2-large-960h-wer-0.7169-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 1.1773063576245661\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_large_960h-wer-1.1773-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 16\n",
      "***normalized\n",
      "wer: 0.7170206636461253\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_large_960h-wer-0.717-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_xls_r_1b_english\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-jonatasgrosman-wav2vec2-xls-r-1b-english-wer-0.5935-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***raw\n",
      "wer: 0.6787402605905847\n",
      "output path: ../results/raw/intron-open-test-facebook_wav2vec2_xls_r_1b_english-wer-0.6787-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.5935363905402928\n",
      "output path: ../results/normalized/intron-open-test-facebook_wav2vec2_xls_r_1b_english-wer-0.5935-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: microsoft_wavlm_libri_clean_100h_base\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-patrickvonplaten-wavlm-libri-clean-100h-base-plus-wer-0.8593-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.8845151574674261\n",
      "output path: ../results/raw/intron-open-test-microsoft_wavlm_libri_clean_100h_base-wer-0.8845-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 74\n",
      "***normalized\n",
      "wer: 0.8593353062881387\n",
      "output path: ../results/normalized/intron-open-test-microsoft_wavlm_libri_clean_100h_base-wer-0.8593-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: microsoft_wavlm_libri_clean_100h_large\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-patrickvonplaten-wavlm-libri-clean-100h-large-wer-0.7051-5474.csv\n",
      "Input shape: (5474, 8)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5474, 8)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.7619852026451909\n",
      "output path: ../results/raw/intron-open-test-microsoft_wavlm_libri_clean_100h_large-wer-0.762-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.7052091244794804\n",
      "output path: ../results/normalized/intron-open-test-microsoft_wavlm_libri_clean_100h_large-wer-0.7052-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: microsoft_azure_speech_api\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-azure-transcribe-wer-0.4437-5498.csv\n",
      "Input shape: (5498, 17)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5498, 17)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.4759248346755713\n",
      "output path: ../results/raw/intron-open-test-microsoft_azure_speech_api-wer-0.4759-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 89\n",
      "***normalized\n",
      "wer: 0.3908676636199356\n",
      "output path: ../results/normalized/intron-open-test-microsoft_azure_speech_api-wer-0.3909-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: google_gcp_speech_api\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-gcp-transcribe-wer-0.6357-5498.csv\n",
      "Input shape: (5498, 17)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5498, 17)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.635906501669613\n",
      "output path: ../results/raw/intron-open-test-google_gcp_speech_api-wer-0.6359-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 170\n",
      "***normalized\n",
      "wer: 0.5777702118743944\n",
      "output path: ../results/normalized/intron-open-test-google_gcp_speech_api-wer-0.5778-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: google_gcp_medical_speech_api\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-gcp-transcribe-medical-wer-0.625-5498.csv\n",
      "Input shape: (5498, 17)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5498, 17)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.6087081778301578\n",
      "output path: ../results/raw/intron-open-test-google_gcp_medical_speech_api-wer-0.6087-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 73\n",
      "***normalized\n",
      "wer: 0.5524840897781735\n",
      "output path: ../results/normalized/intron-open-test-google_gcp_medical_speech_api-wer-0.5525-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: amazon_aws_transcribe_api\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-aws-transcribe-wer-0.5417-5498.csv\n",
      "Input shape: (5498, 17)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5498, 17)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.5539710600405945\n",
      "output path: ../results/raw/intron-open-test-amazon_aws_transcribe_api-wer-0.554-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 15\n",
      "***normalized\n",
      "wer: 0.44231726160856927\n",
      "output path: ../results/normalized/intron-open-test-amazon_aws_transcribe_api-wer-0.4423-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Model name: amazon_aws_transcribe_medical_api_primary_care\n",
      "Dataset split shape: (6346, 14)\n",
      "Dataset split shape (without duplicates): (6316, 14)\n",
      "Input path: ../results/intron-open-test-aws-transcribe-medical-wer-0.5682-5498.csv\n",
      "Input shape: (5498, 17)\n",
      "Reference csv shape: (5474, 8)\n",
      "Reference csv shape (without duplicates): (5471, 21)\n",
      "df shape: (5498, 17)\n",
      "df shape (transformed): (5471, 12)\n",
      "***raw\n",
      "wer: 0.5966738689190074\n",
      "output path: ../results/raw/intron-open-test-amazon_aws_transcribe_medical_api_primary_care-wer-0.5967-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "Total null values: 279\n",
      "***normalized\n",
      "wer: 0.4780792499279784\n",
      "output path: ../results/normalized/intron-open-test-amazon_aws_transcribe_medical_api_primary_care-wer-0.4781-5471.csv\n",
      "Output shape: (5471, 15)\n",
      "==========\n",
      "Combined df shape: (103949, 15)\n",
      "Combined df (normalized) shape: (103949, 15)\n"
     ]
    }
   ],
   "source": [
    "test_combined_df = pd.DataFrame()\n",
    "test_combined_normalized_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model_csv_path in test_model_name_csv_path_dict.items():\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    df, normalized_df = consolidate_zero_shot_results(\n",
    "        model_csv_path=model_csv_path, \n",
    "        model_name=model_name,\n",
    "        metric=wer_metric,\n",
    "        ref_csv_path=\"../results/intron-open-test-whisper_medium-wer-0.3322-5474.csv\",\n",
    "        dataset_csv_path=\"../data/intron-test-public-6346-clean.csv\",\n",
    "        output_dir=\"../results\", \n",
    "    )\n",
    "\n",
    "\n",
    "    test_combined_df = pd.concat([test_combined_df, df])\n",
    "    test_combined_normalized_df = pd.concat([test_combined_normalized_df, normalized_df])\n",
    "    print(\"=\"*10)\n",
    "\n",
    "print(f\"Combined df shape: {test_combined_df.shape}\")\n",
    "print(f\"Combined df (normalized) shape: {test_combined_normalized_df.shape}\")\n",
    "\n",
    "test_combined_df.to_csv(\"../results/raw/intron-open-test-all_models.csv\", index=False)\n",
    "test_combined_normalized_df.to_csv(\"../results/normalized/intron-open-test-all_models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c989a217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>domain</th>\n",
       "      <th>gender</th>\n",
       "      <th>duration</th>\n",
       "      <th>age_group</th>\n",
       "      <th>accent</th>\n",
       "      <th>user_ids</th>\n",
       "      <th>audio_paths</th>\n",
       "      <th>origin</th>\n",
       "      <th>country</th>\n",
       "      <th>reference</th>\n",
       "      <th>prediction</th>\n",
       "      <th>wer</th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149716</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>10.776984</td>\n",
       "      <td>26-40</td>\n",
       "      <td>twi</td>\n",
       "      <td>5ab7b49ad1cab6392a764a69578dc822</td>\n",
       "      <td>/AfriSpeech-100/test/e696aff8-ce05-4c8e-a3b3-5...</td>\n",
       "      <td>african</td>\n",
       "      <td>GH</td>\n",
       "      <td>Proteins break down to release amino acids whi...</td>\n",
       "      <td>Proteins break down to release amino acids whi...</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>openai/whisper-small</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360395</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>12.390000</td>\n",
       "      <td>19-25</td>\n",
       "      <td>igbo</td>\n",
       "      <td>543c037ff44816e8b5ccd0d6cc92fe13</td>\n",
       "      <td>/AfriSpeech-100/test/f28baac3-cdcd-45a2-888c-a...</td>\n",
       "      <td>nigerian</td>\n",
       "      <td>NG</td>\n",
       "      <td>To grant such a patent license to a party mean...</td>\n",
       "      <td>To grant such a patent license to a party mean...</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>openai/whisper-small</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153514</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>5.826984</td>\n",
       "      <td>26-40</td>\n",
       "      <td>isizulu</td>\n",
       "      <td>e87486db0c365bded42184d56b46a2a7</td>\n",
       "      <td>/AfriSpeech-100/test/13062a1b-662b-4afa-85b4-9...</td>\n",
       "      <td>african</td>\n",
       "      <td>ZA</td>\n",
       "      <td>Flatus indicates return of peristalsis.</td>\n",
       "      <td>Fletters indicates return of peristalsis.</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>openai/whisper-small</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129184</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>12.985986</td>\n",
       "      <td>26-40</td>\n",
       "      <td>luganda</td>\n",
       "      <td>9d8db954e680843a47c3b7e224f12371</td>\n",
       "      <td>/AfriSpeech-100/test/7ce32977-b330-43c0-bae0-7...</td>\n",
       "      <td>african</td>\n",
       "      <td>UG</td>\n",
       "      <td>Since the degree of effect produced by a drug ...</td>\n",
       "      <td>Since the degree of effect produced by the dru...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>openai/whisper-small</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155127</td>\n",
       "      <td>clinical</td>\n",
       "      <td>Female</td>\n",
       "      <td>9.730000</td>\n",
       "      <td>26-40</td>\n",
       "      <td>setswana</td>\n",
       "      <td>cdf91cf6e59ee411b985a40a955d4d1f</td>\n",
       "      <td>/AfriSpeech-100/test/27a83595-3d3f-4a6b-b909-7...</td>\n",
       "      <td>african</td>\n",
       "      <td>BW</td>\n",
       "      <td>Protection of the host immune mechanism mighti...</td>\n",
       "      <td>Protection of the host immune mechanism might ...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>openai/whisper-small</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx    domain  gender   duration age_group    accent  \\\n",
       "0  149716  clinical  Female  10.776984     26-40       twi   \n",
       "1  360395  clinical  Female  12.390000     19-25      igbo   \n",
       "2  153514  clinical  Female   5.826984     26-40   isizulu   \n",
       "3  129184  clinical  Female  12.985986     26-40   luganda   \n",
       "4  155127  clinical  Female   9.730000     26-40  setswana   \n",
       "\n",
       "                           user_ids  \\\n",
       "0  5ab7b49ad1cab6392a764a69578dc822   \n",
       "1  543c037ff44816e8b5ccd0d6cc92fe13   \n",
       "2  e87486db0c365bded42184d56b46a2a7   \n",
       "3  9d8db954e680843a47c3b7e224f12371   \n",
       "4  cdf91cf6e59ee411b985a40a955d4d1f   \n",
       "\n",
       "                                         audio_paths    origin country  \\\n",
       "0  /AfriSpeech-100/test/e696aff8-ce05-4c8e-a3b3-5...   african      GH   \n",
       "1  /AfriSpeech-100/test/f28baac3-cdcd-45a2-888c-a...  nigerian      NG   \n",
       "2  /AfriSpeech-100/test/13062a1b-662b-4afa-85b4-9...   african      ZA   \n",
       "3  /AfriSpeech-100/test/7ce32977-b330-43c0-bae0-7...   african      UG   \n",
       "4  /AfriSpeech-100/test/27a83595-3d3f-4a6b-b909-7...   african      BW   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Proteins break down to release amino acids whi...   \n",
       "1  To grant such a patent license to a party mean...   \n",
       "2            Flatus indicates return of peristalsis.   \n",
       "3  Since the degree of effect produced by a drug ...   \n",
       "4  Protection of the host immune mechanism mighti...   \n",
       "\n",
       "                                          prediction       wer  \\\n",
       "0  Proteins break down to release amino acids whi...  0.260870   \n",
       "1  To grant such a patent license to a party mean...  0.120000   \n",
       "2          Fletters indicates return of peristalsis.  0.200000   \n",
       "3  Since the degree of effect produced by the dru...  0.142857   \n",
       "4  Protection of the host immune mechanism might ...  0.466667   \n",
       "\n",
       "                   name split  \n",
       "0  openai/whisper-small  test  \n",
       "1  openai/whisper-small  test  \n",
       "2  openai/whisper-small  test  \n",
       "3  openai/whisper-small  test  \n",
       "4  openai/whisper-small  test  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7b256",
   "metadata": {},
   "source": [
    "# Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f3081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_model_name_csv_path_dict = {\n",
    "    \"openai_whisper_small\": \"../results/intron-open-dev-whisper_small-wer-0.3427-2883.csv\",\n",
    "    \"openai_whisper_small_en\": \"../results/intron-open-dev-whisper_small.en-wer-0.3521-2883.csv\",\n",
    "    \"openai_whisper_medium\": \"../results/intron-open-dev-whisper_medium-wer-0.2735-2883.csv\",\n",
    "    \"openai_whisper_medium_en\": \"../results/intron-open-dev-whisper_medium.en-wer-0.2911-2883.csv\",\n",
    "    \"openai_whisper_large\": \"../results/intron-open-dev-whisper_large-wer-0.2617-2883.csv\",\n",
    "    \"facebook_hubert_large_ls960_ft\": \"../results/intron-open-dev-facebook-hubert-large-ls960-ft-wer-0.5675-2883.csv\",\n",
    "    \"facebook_hubert_xlarge_ls960_ft\": \"../results/intron-open-dev-facebook-hubert-xlarge-ls960-ft-wer-0.571-2883.csv\",\n",
    "    \"facebook_wav2vec2_large_robust_ft_swbd_300h\": \"../results/intron-open-dev-facebook-wav2vec2-large-robust-ft-swbd-300h-wer-0.7338-2883.csv\",\n",
    "    \"facebook_wav2vec2_large_xlsr_53_english\": \"../results/intron-open-dev-jonatasgrosman-wav2vec2-large-xlsr-53-english-wer-0.53-2883.csv\",\n",
    "    \"facebook_wav2vec2_large_960h_lv60_self\": \"../results/intron-open-dev-facebook-wav2vec2-large-960h-lv60-self-wer-0.55-2883.csv\",\n",
    "    \"facebook_wav2vec2_large_960h\": \"../results/intron-open-dev-facebook-wav2vec2-large-960h-wer-0.6528-2883.csv\",\n",
    "    \"facebook_wav2vec2_xls_r_1b_english\": \"../results/intron-open-dev-jonatasgrosman-wav2vec2-xls-r-1b-english-wer-0.5373-2883.csv\",\n",
    "    \"microsoft_wavlm_libri_clean_100h_base\": \"../results/intron-open-dev-patrickvonplaten-wavlm-libri-clean-100h-base-plus-wer-0.8084-2883.csv\",\n",
    "    \"microsoft_wavlm_libri_clean_100h_large\": \"../results/intron-open-dev-patrickvonplaten-wavlm-libri-clean-100h-large-wer-0.6428-2883.csv\",\n",
    "    \"microsoft_azure_speech_api\": \"../results/intron-open-dev-azure-transcribe-wer-0.3729-2887.csv\",\n",
    "    \"google_gcp_speech_api\": \"../results/intron-open-dev-gcp-transcribe-wer-0.5741-2887.csv\",\n",
    "    \"google_gcp_medical_speech_api\": \"../results/intron-open-dev-gcp-transcribe-medical-wer-0.5649-2887.csv\",\n",
    "    \"amazon_aws_transcribe_api\": \"../results/intron-open-dev-aws-transcribe-wer-0.4653-2887.csv\",\n",
    "    \"amazon_aws_transcribe_medical_api_primary_care\": \"../results/intron-open-dev-aws-transcribe-medical-wer-0.4742-2887.csv\",\n",
    "    \"speechbrain_crdnn_rnnlm_librispeech\": \"../results/african-nlp-speechbrain-predictons\",  \n",
    "    \"nvidia_nemo_conformer_ctc_large\": \"../results/african-nlp-nemo-ctc-predictons\",\n",
    "    \"nvidia_nemo_conformer_transducer_large\": \"../results/african-nlp-nemo-transducer-predictons\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce5bae0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: openai_whisper_small\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-whisper_small-wer-0.3427-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.4282729641540881\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_small-wer-0.4283-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.3425525920360631\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_small-wer-0.3426-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: openai_whisper_small_en\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-whisper_small.en-wer-0.3521-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.42191131247212377\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_small_en-wer-0.4219-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.3517092411720511\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_small_en-wer-0.3517-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-whisper_medium-wer-0.2735-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.36815418202305217\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_medium-wer-0.3682-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.27317336589030805\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_medium-wer-0.2732-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: openai_whisper_medium_en\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-whisper_medium.en-wer-0.2911-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.3651728913824268\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_medium_en-wer-0.3652-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.29087622088655146\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_medium_en-wer-0.2909-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: openai_whisper_large\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-whisper_large-wer-0.2617-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.3404070518087279\n",
      "output path: ../results/raw/intron-open-dev-openai_whisper_large-wer-0.3404-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.2612932006010518\n",
      "output path: ../results/normalized/intron-open-dev-openai_whisper_large-wer-0.2613-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_hubert_large_ls960_ft\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-facebook-hubert-large-ls960-ft-wer-0.5675-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 1.1503556421512242\n",
      "output path: ../results/raw/intron-open-dev-facebook_hubert_large_ls960_ft-wer-1.1504-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.5671957175056349\n",
      "output path: ../results/normalized/intron-open-dev-facebook_hubert_large_ls960_ft-wer-0.5672-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_hubert_xlarge_ls960_ft\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-facebook-hubert-xlarge-ls960-ft-wer-0.571-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 1.1665532054743069\n",
      "output path: ../results/raw/intron-open-dev-facebook_hubert_xlarge_ls960_ft-wer-1.1666-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.5707879413974455\n",
      "output path: ../results/normalized/intron-open-dev-facebook_hubert_xlarge_ls960_ft-wer-0.5708-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_robust_ft_swbd_300h\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-facebook-wav2vec2-large-robust-ft-swbd-300h-wer-0.7338-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 1.2077278809361722\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_large_robust_ft_swbd_300h-wer-1.2077-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 3\n",
      "***normalized\n",
      "wer: 0.7336823816679189\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_large_robust_ft_swbd_300h-wer-0.7337-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_xlsr_53_english\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-jonatasgrosman-wav2vec2-large-xlsr-53-english-wer-0.53-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.5956947346181836\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english-wer-0.5957-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.5298178061607813\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_large_xlsr_53_english-wer-0.5298-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_960h_lv60_self\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-facebook-wav2vec2-large-960h-lv60-self-wer-0.55-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 1.1571163642339022\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_large_960h_lv60_self-wer-1.1571-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 2\n",
      "***normalized\n",
      "wer: 0.5497511269722013\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_large_960h_lv60_self-wer-0.5498-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_large_960h\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-facebook-wav2vec2-large-960h-wer-0.6528-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 1.1657785394023334\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_large_960h-wer-1.1658-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.6524464688204358\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_large_960h-wer-0.6524-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: facebook_wav2vec2_xls_r_1b_english\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-jonatasgrosman-wav2vec2-xls-r-1b-english-wer-0.5373-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***raw\n",
      "wer: 0.6213995633700322\n",
      "output path: ../results/raw/intron-open-dev-facebook_wav2vec2_xls_r_1b_english-wer-0.6214-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 0\n",
      "***normalized\n",
      "wer: 0.5371431254695718\n",
      "output path: ../results/normalized/intron-open-dev-facebook_wav2vec2_xls_r_1b_english-wer-0.5371-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: microsoft_wavlm_libri_clean_100h_base\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-patrickvonplaten-wavlm-libri-clean-100h-base-plus-wer-0.8084-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.8377661447451819\n",
      "output path: ../results/raw/intron-open-dev-microsoft_wavlm_libri_clean_100h_base-wer-0.8378-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 31\n",
      "***normalized\n",
      "wer: 0.8082738542449286\n",
      "output path: ../results/normalized/intron-open-dev-microsoft_wavlm_libri_clean_100h_base-wer-0.8083-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: microsoft_wavlm_libri_clean_100h_large\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-patrickvonplaten-wavlm-libri-clean-100h-large-wer-0.6428-2883.csv\n",
      "Input shape: (2883, 8)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2883, 8)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.7051808727904411\n",
      "output path: ../results/raw/intron-open-dev-microsoft_wavlm_libri_clean_100h_large-wer-0.7052-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.6426793764087152\n",
      "output path: ../results/normalized/intron-open-dev-microsoft_wavlm_libri_clean_100h_large-wer-0.6427-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: microsoft_azure_speech_api\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-azure-transcribe-wer-0.3729-2887.csv\n",
      "Input shape: (2887, 17)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2887, 17)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.41883612291368344\n",
      "output path: ../results/raw/intron-open-dev-microsoft_azure_speech_api-wer-0.4188-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.3343116078136739\n",
      "output path: ../results/normalized/intron-open-dev-microsoft_azure_speech_api-wer-0.3343-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: google_gcp_speech_api\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-gcp-transcribe-wer-0.5741-2887.csv\n",
      "Input shape: (2887, 17)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2887, 17)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.589708678607479\n",
      "output path: ../results/raw/intron-open-dev-google_gcp_speech_api-wer-0.5897-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 85\n",
      "***normalized\n",
      "wer: 0.5299821562734786\n",
      "output path: ../results/normalized/intron-open-dev-google_gcp_speech_api-wer-0.53-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: google_gcp_medical_speech_api\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-gcp-transcribe-medical-wer-0.5649-2887.csv\n",
      "Input shape: (2887, 17)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2887, 17)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.5639099509378155\n",
      "output path: ../results/raw/intron-open-dev-google_gcp_medical_speech_api-wer-0.5639-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 36\n",
      "***normalized\n",
      "wer: 0.512396694214876\n",
      "output path: ../results/normalized/intron-open-dev-google_gcp_medical_speech_api-wer-0.5124-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: amazon_aws_transcribe_api\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-aws-transcribe-wer-0.4653-2887.csv\n",
      "Input shape: (2887, 17)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2887, 17)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.4984389304913261\n",
      "output path: ../results/raw/intron-open-dev-amazon_aws_transcribe_api-wer-0.4984-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 7\n",
      "***normalized\n",
      "wer: 0.38488448534936137\n",
      "output path: ../results/normalized/intron-open-dev-amazon_aws_transcribe_api-wer-0.3849-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: amazon_aws_transcribe_medical_api_primary_care\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/intron-open-dev-aws-transcribe-medical-wer-0.4742-2887.csv\n",
      "Input shape: (2887, 17)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (2887, 17)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.522101457780699\n",
      "output path: ../results/raw/intron-open-dev-amazon_aws_transcribe_medical_api_primary_care-wer-0.5221-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.4002864387678437\n",
      "output path: ../results/normalized/intron-open-dev-amazon_aws_transcribe_medical_api_primary_care-wer-0.4003-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: speechbrain_crdnn_rnnlm_librispeech\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/african-nlp-speechbrain-predictons\n",
      "Input shape: (3227, 2)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (3227, 2)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 1.2844432967910044\n",
      "output path: ../results/raw/intron-open-dev-speechbrain_crdnn_rnnlm_librispeech-wer-1.2844-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 1\n",
      "***normalized\n",
      "wer: 0.8594102178812922\n",
      "output path: ../results/normalized/intron-open-dev-speechbrain_crdnn_rnnlm_librispeech-wer-0.8594-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: nvidia_nemo_conformer_ctc_large\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/african-nlp-nemo-ctc-predictons\n",
      "Input shape: (3227, 2)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (3227, 2)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.5574778750674898\n",
      "output path: ../results/raw/intron-open-dev-nvidia_nemo_conformer_ctc_large-wer-0.5575-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 2\n",
      "***normalized\n",
      "wer: 0.4483236288504884\n",
      "output path: ../results/normalized/intron-open-dev-nvidia_nemo_conformer_ctc_large-wer-0.4483-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Model name: nvidia_nemo_conformer_transducer_large\n",
      "Dataset split shape: (3231, 14)\n",
      "Dataset split shape (without duplicates): (3227, 14)\n",
      "Input path: ../results/african-nlp-nemo-transducer-predictons\n",
      "Input shape: (3227, 2)\n",
      "Reference csv shape: (2872, 8)\n",
      "Reference csv shape (without duplicates): (2872, 21)\n",
      "df shape: (3227, 2)\n",
      "df shape (transformed): (2872, 12)\n",
      "***raw\n",
      "wer: 0.563675203643278\n",
      "output path: ../results/raw/intron-open-dev-nvidia_nemo_conformer_transducer_large-wer-0.5637-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "Total null values: 6\n",
      "***normalized\n",
      "wer: 0.4429470323065364\n",
      "output path: ../results/normalized/intron-open-dev-nvidia_nemo_conformer_transducer_large-wer-0.4429-2872.csv\n",
      "Output shape: (2872, 15)\n",
      "==========\n",
      "Combined df shape: (63184, 15)\n",
      "Combined df (normalized) shape: (63184, 15)\n"
     ]
    }
   ],
   "source": [
    "dev_combined_df = pd.DataFrame()\n",
    "dev_combined_normalized_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model_csv_path in dev_model_name_csv_path_dict.items():\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    df, normalized_df = consolidate_zero_shot_results(\n",
    "        model_csv_path=model_csv_path, \n",
    "        model_name=model_name,\n",
    "        metric=wer_metric,\n",
    "        ref_csv_path=\"../results/intron-open-dev-whisper_medium-wer-0.3049-2872.csv\",\n",
    "        dataset_csv_path=\"../data/intron-dev-public-3231-clean.csv\",\n",
    "        output_dir=\"../results\", \n",
    "    )\n",
    "\n",
    "    dev_combined_df = pd.concat([dev_combined_df, df])\n",
    "    dev_combined_normalized_df = pd.concat([dev_combined_normalized_df, normalized_df])\n",
    "    print(\"=\"*10)\n",
    "\n",
    "print(f\"Combined df shape: {dev_combined_df.shape}\")\n",
    "print(f\"Combined df (normalized) shape: {dev_combined_normalized_df.shape}\")\n",
    "\n",
    "dev_combined_df.to_csv(\"../results/raw/intron-open-dev-all_models.csv\", index=False)\n",
    "dev_combined_normalized_df.to_csv(\"../results/normalized/intron-open-dev-all_models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143653ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
